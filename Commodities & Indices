# --- Enhanced Multi-Asset Market Movement Detection & Forecasting System V2 ---
# Predicts: S&P 500, NASDAQ, Oil (CL=F), Gold with ML, regime detection, and macro signals
# FIXED: All USO references removed, uses CL=F (WTI Crude Oil Futures) directly

import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

print("Installing required packages...")
!pip install yfinance arch pandas numpy scipy scikit-learn statsmodels xgboost lightgbm --quiet

import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import sys
from scipy import stats
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import train_test_split, cross_val_score
from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression
from statsmodels.tsa.stattools import adfuller, acf
import xgboost as xgb
import lightgbm as lgb
from arch import arch_model
import math

# ============================================================================
# SECTION 1: DATA COLLECTION - ASSETS & MACRO INDICATORS
# ============================================================================

print("=" * 80)
print("MULTI-ASSET MARKET MOVEMENT PREDICTION SYSTEM V2")
print("=" * 80)

print("\n[1/12] Collecting market data...")

end_date = datetime.utcnow()
start_date = end_date - timedelta(days=5*365)  # 5 years of data

# Define primary assets to predict - NO USO ANYWHERE
assets = {
    'SPY': 'S&P 500 ETF',
    'QQQ': 'NASDAQ-100 ETF',
    'CL=F': 'WTI Crude Oil Futures',
    'GLD': 'Gold ETF'
}

# Backup tickers if primary fails - NO USO
asset_backups = {
    'CL=F': ['BZ=F'],  # Only Brent futures as backup
}

# Download primary asset data
print("  Fetching primary assets...")
asset_data = {}
failed_assets = []

for ticker, name in assets.items():
    try:
        print(f"    Downloading {ticker}...", end='')
        data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=False)
        
        # Handle both single ticker and multi-ticker returns
        if isinstance(data.columns, pd.MultiIndex):
            data = data.xs(ticker, axis=1, level=1)
        
        if not data.empty and len(data) > 100:
            asset_data[ticker] = data
            print(f" ‚úì {name} ({len(data)} days)")
        else:
            print(f" ‚úó Insufficient data")
            failed_assets.append(ticker)
    except Exception as e:
        print(f" ‚úó Failed: {e}")
        failed_assets.append(ticker)

# Try backup tickers for failed assets
if failed_assets:
    print("\n  Trying backup tickers...")
    for failed_ticker in failed_assets:
        if failed_ticker in asset_backups:
            for backup_ticker in asset_backups[failed_ticker]:
                try:
                    print(f"    Downloading {backup_ticker} (backup for {failed_ticker})...", end='')
                    data = yf.download(backup_ticker, start=start_date, end=end_date, progress=False, auto_adjust=False)
                    
                    if isinstance(data.columns, pd.MultiIndex):
                        data = data.xs(backup_ticker, axis=1, level=1)
                    
                    if not data.empty and len(data) > 100:
                        asset_data[failed_ticker] = data
                        assets[failed_ticker] = f"{assets[failed_ticker]} ({backup_ticker})"
                        print(f" ‚úì Using {backup_ticker} as backup ({len(data)} days)")
                        break
                    else:
                        print(f" ‚úó Insufficient data")
                except Exception as e:
                    print(f" ‚úó Failed: {e}")

# Download market indicators - NO USO
print("\n  Fetching market indicators...")
indicators = {
    '^VIX': 'VIX (Volatility Index)',
    '^TNX': '10-Year Treasury Yield',
    'DX-Y.NYB': 'US Dollar Index',
    'TLT': 'Long-Term Treasury ETF',
    'HYG': 'High Yield Corporate Bond ETF',
    'LQD': 'Investment Grade Corporate Bond ETF',
    'EEM': 'Emerging Markets ETF',
    'IWM': 'Russell 2000 Small Cap ETF',
    'XLE': 'Energy Sector ETF',
    'XLF': 'Financial Sector ETF',
    'XLK': 'Technology Sector ETF',
    'GDX': 'Gold Miners ETF',
    'UNG': 'Natural Gas ETF',
    'BTC-USD': 'Bitcoin',
    'ETH-USD': 'Ethereum',
    'BZ=F': 'Brent Oil Futures',
    'NG=F': 'Natural Gas Futures',
    'XOP': 'Oil & Gas Exploration ETF',
    'OIH': 'Oil Services ETF'
}

indicator_data = {}
for ticker, name in indicators.items():
    try:
        data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=False)
        
        if isinstance(data.columns, pd.MultiIndex):
            data = data.xs(ticker, axis=1, level=1)
        
        if not data.empty and len(data) > 100:
            indicator_data[ticker] = data
            print(f"    ‚úì {name}")
    except Exception as e:
        continue

if not asset_data:
    print("\n‚ùå ERROR: Failed to download any asset data!")
    raise SystemExit("Cannot proceed without data")

# Display which oil ticker is being used
oil_ticker_used = None
for ticker in asset_data.keys():
    if ticker in ['CL=F', 'BZ=F']:
        oil_ticker_used = ticker
        break

if oil_ticker_used:
    # Get current oil price
    current_oil_price = float(asset_data[oil_ticker_used]['Close'].iloc[-1])
    oil_source = "WTI Crude Oil Futures (CL=F)" if oil_ticker_used == 'CL=F' else "Brent Oil Futures (BZ=F)"
    print(f"\nüí° Oil Data Source: {oil_source}")
    print(f"üí∞ Current Oil Price: ${current_oil_price:.2f}")

print(f"\n‚úì Collected data for {len(asset_data)} primary assets and {len(indicator_data)} indicators")

# ============================================================================
# SECTION 2: DATA PREPROCESSING & FEATURE ENGINEERING
# ============================================================================

print("\n[2/12] Preprocessing and engineering features...")

def extract_ohlcv(data_dict):
    """Extract OHLCV data from yfinance results"""
    if not data_dict:
        empty_df = pd.DataFrame()
        return empty_df, empty_df, empty_df, empty_df, empty_df
    
    closes_dict = {}
    opens_dict = {}
    highs_dict = {}
    lows_dict = {}
    volumes_dict = {}
    
    for ticker, data in data_dict.items():
        try:
            if 'Close' in data.columns:
                closes_dict[ticker] = data['Close']
            if 'Open' in data.columns:
                opens_dict[ticker] = data['Open']
            if 'High' in data.columns:
                highs_dict[ticker] = data['High']
            if 'Low' in data.columns:
                lows_dict[ticker] = data['Low']
            if 'Volume' in data.columns:
                volumes_dict[ticker] = data['Volume']
        except Exception as e:
            continue
    
    closes = pd.DataFrame(closes_dict) if closes_dict else pd.DataFrame()
    opens = pd.DataFrame(opens_dict) if opens_dict else pd.DataFrame()
    highs = pd.DataFrame(highs_dict) if highs_dict else pd.DataFrame()
    lows = pd.DataFrame(lows_dict) if lows_dict else pd.DataFrame()
    volumes = pd.DataFrame(volumes_dict) if volumes_dict else pd.DataFrame()
    
    return closes, opens, highs, lows, volumes

print("  Extracting OHLCV data...")
asset_closes, asset_opens, asset_highs, asset_lows, asset_volumes = extract_ohlcv(asset_data)
indicator_closes, _, _, _, _ = extract_ohlcv(indicator_data)

if asset_closes.empty:
    raise ValueError("Failed to extract any price data")

print(f"  ‚úì Extracted data for {len(asset_closes.columns)} assets")

# Align all data
all_closes = pd.concat([asset_closes, indicator_closes], axis=1)
all_closes = all_closes.ffill(limit=5).dropna(how='all')

# Calculate returns
returns = all_closes.pct_change().dropna()
log_returns = np.log(all_closes / all_closes.shift(1)).dropna()

print(f"‚úì Data spans {len(all_closes)} trading days")

# ============================================================================
# SECTION 3: TARGET VARIABLE CREATION
# ============================================================================

print("\n[3/12] Creating prediction targets...")

horizons = {
    '1d': 1,
    '3d': 3,
    '5d': 5,
    '10d': 10,
    '20d': 20
}

targets = {}

for ticker in assets.keys():
    targets[ticker] = {}
    
    for horizon_name, days in horizons.items():
        forward_return = asset_closes[ticker].pct_change(days).shift(-days)
        
        targets[ticker][f'{horizon_name}_direction'] = pd.cut(
            forward_return * 100,
            bins=[-np.inf, -0.5, 0.5, np.inf],
            labels=[-1, 0, 1]
        ).astype(float)
        
        targets[ticker][f'{horizon_name}_binary'] = (forward_return > 0).astype(int)
        targets[ticker][f'{horizon_name}_return'] = forward_return

for ticker in assets.keys():
    targets[ticker] = pd.DataFrame(targets[ticker])

print(f"‚úì Created targets for {len(horizons)} prediction horizons")

# ============================================================================
# SECTION 4: COMPREHENSIVE FEATURE ENGINEERING
# ============================================================================

print("\n[4/12] Engineering comprehensive features...")

features_dict = {}

for ticker in assets.keys():
    features = pd.DataFrame(index=asset_closes.index)
    price = asset_closes[ticker]
    
    # Returns
    for period in [1, 2, 3, 5, 10, 20, 60]:
        features[f'return_{period}d'] = price.pct_change(period)
        features[f'log_return_{period}d'] = np.log(price / price.shift(period))
    
    # Moving averages
    for window in [5, 10, 20, 50, 100, 200]:
        features[f'sma_{window}'] = price.rolling(window).mean()
        features[f'price_to_sma_{window}'] = price / features[f'sma_{window}']
        features[f'sma_{window}_slope'] = features[f'sma_{window}'].diff(5)
    
    # EMA
    for span in [12, 26, 50]:
        features[f'ema_{span}'] = price.ewm(span=span).mean()
        features[f'price_to_ema_{span}'] = price / features[f'ema_{span}']
    
    # Moving average crossovers
    features['sma_cross_50_200'] = features['sma_50'] - features['sma_200']
    features['ema_cross_12_26'] = features['ema_12'] - features['ema_26']
    
    # RSI
    for period in [7, 14, 21, 28]:
        delta = price.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss.replace(0, np.nan)
        rsi_values = 100 - (100 / (1 + rs))
        rsi_values = rsi_values.fillna(50)
        features[f'rsi_{period}'] = rsi_values
    
    # MACD
    ema_12 = price.ewm(span=12).mean()
    ema_26 = price.ewm(span=26).mean()
    features['macd'] = ema_12 - ema_26
    features['macd_signal'] = features['macd'].ewm(span=9).mean()
    features['macd_hist'] = features['macd'] - features['macd_signal']
    
    # Stochastic
    for period in [14, 21]:
        low_min = asset_lows[ticker].rolling(window=period).min()
        high_max = asset_highs[ticker].rolling(window=period).max()
        features[f'stoch_{period}'] = 100 * (price - low_min) / (high_max - low_min)
        features[f'stoch_{period}_ma'] = features[f'stoch_{period}'].rolling(3).mean()
    
    # ROC
    for period in [5, 10, 20]:
        features[f'roc_{period}'] = ((price - price.shift(period)) / price.shift(period)) * 100
    
    # Volatility
    for window in [5, 10, 20, 60]:
        features[f'volatility_{window}d'] = returns[ticker].rolling(window).std() * np.sqrt(252)
    
    # Bollinger Bands
    for window in [20, 50]:
        sma = price.rolling(window).mean()
        std = price.rolling(window).std()
        features[f'bb_upper_{window}'] = sma + 2 * std
        features[f'bb_lower_{window}'] = sma - 2 * std
        features[f'bb_width_{window}'] = (features[f'bb_upper_{window}'] - features[f'bb_lower_{window}']) / sma
        features[f'bb_position_{window}'] = (price - features[f'bb_lower_{window}']) / (features[f'bb_upper_{window}'] - features[f'bb_lower_{window}'])
    
    # ATR
    high = asset_highs[ticker]
    low = asset_lows[ticker]
    close_prev = price.shift(1)
    
    tr1 = high - low
    tr2 = abs(high - close_prev)
    tr3 = abs(low - close_prev)
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    
    for period in [7, 14, 21]:
        atr_val = tr.rolling(period).mean()
        features[f'atr_{period}'] = atr_val
        features[f'atr_{period}_pct'] = (atr_val / price.replace(0, np.nan)).fillna(0.02) * 100
    
    # Volume indicators
    volume = asset_volumes[ticker]
    
    for window in [5, 10, 20]:
        features[f'volume_sma_{window}'] = volume.rolling(window).mean()
        features[f'volume_ratio_{window}'] = volume / features[f'volume_sma_{window}']
    
    # OBV
    obv = (np.sign(price.diff()) * volume).fillna(0).cumsum()
    features['obv'] = obv
    features['obv_ema'] = obv.ewm(span=20).mean()
    
    # VPT
    features['vpt'] = (volume * ((price - price.shift(1)) / price.shift(1))).cumsum()
    
    # Pattern recognition
    features['higher_high'] = (high > high.shift(1)).astype(int)
    features['higher_low'] = (low > low.shift(1)).astype(int)
    features['uptrend_strength'] = features['higher_high'].rolling(5).sum() + features['higher_low'].rolling(5).sum()
    
    # Price channels
    features['channel_high_20'] = high.rolling(20).max()
    features['channel_low_20'] = low.rolling(20).min()
    features['channel_position'] = (price - features['channel_low_20']) / (features['channel_high_20'] - features['channel_low_20'])
    
    # Distance from 52-week high/low
    features['dist_from_52w_high'] = (price / high.rolling(252).max() - 1) * 100
    features['dist_from_52w_low'] = (price / low.rolling(252).min() - 1) * 100
    
    # Statistical features
    for window in [20, 60]:
        features[f'return_skew_{window}'] = returns[ticker].rolling(window).skew()
        features[f'return_kurt_{window}'] = returns[ticker].rolling(window).kurt()
    
    # Z-score
    for window in [20, 60]:
        mean = price.rolling(window).mean()
        std = price.rolling(window).std()
        features[f'zscore_{window}'] = (price - mean) / std
    
    features_dict[ticker] = features

print(f"‚úì Created {len(features_dict[list(assets.keys())[0]].columns)} features per asset")

# ============================================================================
# SECTION 5: CROSS-ASSET & MACRO FEATURES (NO USO)
# ============================================================================

print("\n[5/12] Creating cross-asset and macro features...")

market_features = pd.DataFrame(index=all_closes.index)

# VIX features
if '^VIX' in indicator_closes.columns:
    vix = indicator_closes['^VIX']
    market_features['vix'] = vix
    market_features['vix_sma_20'] = vix.rolling(20).mean()
    market_features['vix_change'] = vix.pct_change()
    market_features['vix_spike'] = (vix > vix.rolling(20).mean() + vix.rolling(20).std()).astype(int)

# Treasury yield
if '^TNX' in indicator_closes.columns:
    tnx = indicator_closes['^TNX']
    market_features['yield_10y'] = tnx
    market_features['yield_10y_change'] = tnx.diff()
    market_features['yield_10y_roc'] = tnx.pct_change(20)

# Dollar Index
if 'DX-Y.NYB' in indicator_closes.columns:
    dxy = indicator_closes['DX-Y.NYB']
    market_features['dxy'] = dxy
    market_features['dxy_return_20d'] = dxy.pct_change(20)
    market_features['dxy_sma_50'] = dxy.rolling(50).mean()

# Credit spreads
if 'HYG' in indicator_closes.columns and 'LQD' in indicator_closes.columns:
    hyg_ret = indicator_closes['HYG'].pct_change(20)
    lqd_ret = indicator_closes['LQD'].pct_change(20)
    market_features['credit_spread_proxy'] = hyg_ret - lqd_ret

# Market breadth
if 'IWM' in indicator_closes.columns:
    spy_ret = asset_closes['SPY'].pct_change(20)
    iwm_ret = indicator_closes['IWM'].pct_change(20)
    market_features['large_vs_small_cap'] = spy_ret - iwm_ret

# Sector rotations
sector_etfs = ['XLE', 'XLF', 'XLK']
for sector in sector_etfs:
    if sector in indicator_closes.columns:
        sector_ret = indicator_closes[sector].pct_change(20)
        market_features[f'{sector}_momentum'] = sector_ret

# Crypto correlation
if 'BTC-USD' in indicator_closes.columns:
    btc_ret = indicator_closes['BTC-USD'].pct_change(20)
    market_features['btc_momentum'] = btc_ret

# Inter-asset correlations - NO USO REFERENCES
oil_ticker = None
for potential_oil in ['CL=F', 'BZ=F']:
    if potential_oil in returns.columns:
        oil_ticker = potential_oil
        break

# Calculate correlations based on available tickers
if 'SPY' in returns.columns and 'QQQ' in returns.columns:
    market_features['spy_qqq_corr'] = returns['SPY'].rolling(60).corr(returns['QQQ'])

if 'SPY' in returns.columns and 'GLD' in returns.columns:
    market_features['spy_gld_corr'] = returns['SPY'].rolling(60).corr(returns['GLD'])

if oil_ticker and 'SPY' in returns.columns:
    market_features['spy_oil_corr'] = returns['SPY'].rolling(60).corr(returns[oil_ticker])

if oil_ticker and 'GLD' in returns.columns:
    market_features['oil_gld_corr'] = returns[oil_ticker].rolling(60).corr(returns['GLD'])

if 'QQQ' in returns.columns and 'GLD' in returns.columns:
    market_features['qqq_gld_corr'] = returns['QQQ'].rolling(60).corr(returns['GLD'])

if oil_ticker and 'QQQ' in returns.columns:
    market_features['qqq_oil_corr'] = returns['QQQ'].rolling(60).corr(returns[oil_ticker])

# Add market features to each asset
for ticker in assets.keys():
    features_dict[ticker] = pd.concat([features_dict[ticker], market_features], axis=1)

print(f"‚úì Added {len(market_features.columns)} market-wide features")

print(f"\n‚úÖ All USO references removed - using CL=F (WTI Crude Oil Futures) at ${current_oil_price:.2f}")

# ============================================================================
# SECTION 6: REGIME DETECTION
# ============================================================================

print("\n[6/12] Detecting market regimes...")

regime_features = pd.DataFrame(index=all_closes.index)

try:
    spy_returns = returns['SPY'] * 100
    
    model = MarkovRegression(
        spy_returns.dropna(),
        k_regimes=3,
        switching_variance=True
    )
    
    res = model.fit(maxiter=100, disp=False, warn_convergence=False)
    
    smoothed = res.smoothed_marginal_probabilities
    regime_features['regime_0_prob'] = smoothed[0]
    regime_features['regime_1_prob'] = smoothed[1]
    regime_features['regime_2_prob'] = smoothed[2]
    regime_features['regime_current'] = smoothed.idxmax(axis=1)
    
    for i in range(3):
        vol = np.sqrt(res.params[f'sigma2[{i}]'])
        regime_features[f'regime_{i}_vol'] = vol
    
    print("  ‚úì Markov Regime-Switching model fitted")
    
except Exception as e:
    print(f"  Note: Using volatility-based regime detection: {e}")
    vol = returns['SPY'].rolling(20).std()
    vol_z = (vol - vol.rolling(60).mean()) / vol.rolling(60).std()
    
    regime_features['regime_current'] = pd.cut(
        vol_z,
        bins=[-np.inf, -0.5, 0.5, np.inf],
        labels=[0, 1, 2]
    ).astype(float)

for ticker in assets.keys():
    features_dict[ticker] = pd.concat([features_dict[ticker], regime_features], axis=1)

print(f"‚úì Added regime detection features")

# ============================================================================
# SECTION 7: PREPARE ML DATASETS
# ============================================================================

print("\n[7/12] Preparing machine learning datasets...")

def prepare_ml_dataset(ticker, horizon='5d', target_type='binary'):
    """Prepare features and targets for ML training"""
    
    X = features_dict[ticker].copy()
    
    target_col = f'{horizon}_{target_type}'
    if target_col not in targets[ticker].columns:
        raise ValueError(f"Target {target_col} not found for {ticker}")
    
    y = targets[ticker][target_col]
    
    data = pd.concat([X, y.rename('target')], axis=1)
    data = data.dropna(subset=['target'])
    
    if len(data) == 0:
        raise ValueError(f"No valid data after alignment for {ticker} - {horizon}")
    
    X_clean = data.drop('target', axis=1)
    y_clean = data['target']
    
    X_clean = X_clean.replace([np.inf, -np.inf], np.nan)
    
    for col in X_clean.columns:
        if X_clean[col].isna().all():
            X_clean[col] = 0
        else:
            X_clean[col] = X_clean[col].fillna(X_clean[col].median())
    
    if X_clean.isna().any().any():
        X_clean = X_clean.fillna(0)
    
    return X_clean, y_clean

ml_datasets = {}

for ticker in assets.keys():
    try:
        print(f"  Preparing {ticker}...", end='')
        X, y = prepare_ml_dataset(ticker, horizon='5d', target_type='binary')
        
        if len(X) < 100:
            print(f" ‚ö† Insufficient data ({len(X)} samples) - skipping")
            continue
        
        ml_datasets[ticker] = {'X': X, 'y': y}
        print(f" ‚úì {len(X)} samples, {X.shape[1]} features")
        
    except Exception as e:
        print(f" ‚úó Error: {e}")
        continue

if not ml_datasets:
    print("\n‚ùå ERROR: Failed to prepare any ML datasets!")
    raise SystemExit("Cannot proceed without valid datasets")

print(f"\n‚úì Prepared ML datasets for {len(ml_datasets)} assets")

# ============================================================================
# SECTION 8: TRAIN ML MODELS
# ============================================================================

print("\n[8/12] Training machine learning models...")

models = {}
predictions = {}
performance = {}

for ticker in assets.keys():
    if ticker not in ml_datasets:
        print(f"  ‚ö† Skipping {ticker} - no valid dataset")
        continue
    
    print(f"\n  Training models for {ticker} ({assets[ticker]})...")
    
    X = ml_datasets[ticker]['X']
    y = ml_datasets[ticker]['y']
    
    if len(X) < 100:
        print(f"    ‚ö† Insufficient data ({len(X)} samples) - skipping")
        continue
    
    train_size = int(len(X) * 0.7)
    
    if train_size < 50:
        print(f"    ‚ö† Training set too small ({train_size} samples) - skipping")
        continue
    
    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]
    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]
    
    if len(X_train) == 0 or len(X_test) == 0:
        print(f"    ‚ö† Empty train or test set - skipping")
        continue
    
    print(f"    Train: {len(X_train)} samples, Test: {len(X_test)} samples")
    
    scaler = RobustScaler()
    try:
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
    except Exception as e:
        print(f"    ‚ö† Scaling failed: {e} - skipping")
        continue
    
    model_dict = {}
    
    # Logistic Regression
    try:
        lr = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
        lr.fit(X_train_scaled, y_train)
        model_dict['LogisticRegression'] = lr
    except:
        pass
    
    # Random Forest
    try:
        rf = RandomForestClassifier(
            n_estimators=300,
            max_depth=20,
            min_samples_split=10,
            min_samples_leaf=5,
            max_features='sqrt',
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        )
        rf.fit(X_train_scaled, y_train)
        model_dict['RandomForest'] = rf
    except:
        pass
    
    # Gradient Boosting
    try:
        gb = GradientBoostingClassifier(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.03,
            subsample=0.8,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42
        )
        gb.fit(X_train_scaled, y_train)
        model_dict['GradientBoosting'] = gb
    except:
        pass
    
    # XGBoost
    try:
        xgb_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )
        xgb_model.fit(X_train_scaled, y_train)
        model_dict['XGBoost'] = xgb_model
    except:
        pass
    
    # LightGBM
    try:
        lgb_model = lgb.LGBMClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            verbose=-1
        )
        lgb_model.fit(X_train_scaled, y_train)
        model_dict['LightGBM'] = lgb_model
    except:
        pass
    
    # Ensemble
    if len(model_dict) >= 3:
        voting_clf = VotingClassifier(
            estimators=[(name, model) for name, model in model_dict.items()],
            voting='soft'
        )
        voting_clf.fit(X_train_scaled, y_train)
        model_dict['Ensemble'] = voting_clf
    
    # Evaluate models
    results = {}
    for name, model in model_dict.items():
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None
        
        accuracy = accuracy_score(y_test, y_pred)
        
        results[name] = {
            'accuracy': accuracy,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
        
        print(f"    {name}: {accuracy:.2%} accuracy")
    
    best_model_name = max(results, key=lambda x: results[x]['accuracy'])
    best_model = model_dict[best_model_name]
    
    print(f"    ‚Üí Best: {best_model_name} ({results[best_model_name]['accuracy']:.2%})")
    
    models[ticker] = {
        'scaler': scaler,
        'best_model': best_model,
        'best_model_name': best_model_name,
        'all_models': model_dict,
        'feature_names': X.columns.tolist()
    }
    
    predictions[ticker] = {
        'y_test': y_test,
        'y_pred': results[best_model_name]['y_pred'],
        'y_pred_proba': results[best_model_name]['y_pred_proba'],
        'X_test_index': X_test.index
    }
    
    performance[ticker] = {
        'accuracy': results[best_model_name]['accuracy'],
        'model_used': best_model_name
    }

print(f"\n‚úì Trained models for all {len(models)} assets")

# ============================================================================
# SECTION 9: FEATURE IMPORTANCE ANALYSIS
# ============================================================================

print("\n[9/12] Analyzing feature importance...")

feature_importance_dict = {}

for ticker in assets.keys():
    if ticker not in models:
        continue
    
    model = models[ticker]['best_model']
    feature_names = models[ticker]['feature_names']
    
    if hasattr(model, 'feature_importances_'):
        importance = model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        feature_importance_dict[ticker] = feature_importance_df
        
        print(f"\n  Top 10 features for {ticker}:")
        print(feature_importance_df.head(10)[['feature', 'importance']].to_string(index=False))

for ticker, df in feature_importance_dict.items():
    df.to_csv(f'feature_importance_{ticker}.csv', index=False)

# ============================================================================
# SECTION 10: GENERATE PREDICTIONS
# ============================================================================

print("\n[10/12] Generating forward predictions...")

forward_predictions = {}

for ticker in assets.keys():
    if ticker not in models:
        print(f"  ‚ö† No trained model for {ticker} - skipping prediction")
        continue
    
    try:
        latest_features = features_dict[ticker].iloc[-1:].copy()
        latest_features = latest_features.replace([np.inf, -np.inf], np.nan)
        
        for col in latest_features.columns:
            if latest_features[col].isna().all() or pd.isna(latest_features[col].iloc[0]):
                median_val = features_dict[ticker][col].median()
                if pd.isna(median_val):
                    latest_features[col] = 0
                else:
                    latest_features[col] = median_val
        
        scaler = models[ticker]['scaler']
        latest_scaled = scaler.transform(latest_features)
        
        model = models[ticker]['best_model']
        pred_class = model.predict(latest_scaled)[0]
        pred_proba = model.predict_proba(latest_scaled)[0] if hasattr(model, 'predict_proba') else None
        
        current_price = float(asset_closes[ticker].iloc[-1])
        
        forward_predictions[ticker] = {
            'current_price': current_price,
            'prediction_class': int(pred_class),
            'prediction_direction': 'UP' if pred_class == 1 else 'DOWN/FLAT',
            'probability_up': float(pred_proba[1]) if pred_proba is not None else None,
            'probability_down': float(pred_proba[0]) if pred_proba is not None else None,
            'confidence': float(max(pred_proba)) if pred_proba is not None else None,
            'model_used': models[ticker]['best_model_name']
        }
    except Exception as e:
        print(f"  ‚ö† Prediction failed for {ticker}: {e}")
        continue

if not forward_predictions:
    print("\n‚ö† WARNING: No forward predictions generated!")
else:
    print("\n" + "=" * 80)
    print("PREDICTIONS FOR NEXT 5 DAYS")
    print("=" * 80)

    for ticker, pred in forward_predictions.items():
        ticker_name = assets[ticker]
        if ticker == oil_ticker:
            ticker_name += f" - Current: ${pred['current_price']:.2f}/barrel"
        
        print(f"\n{ticker} ({ticker_name}):")
        print(f"  Current Price: ${pred['current_price']:.2f}")
        print(f"  Prediction: {pred['prediction_direction']}")
        if pred['probability_up'] is not None:
            print(f"  Probability UP: {pred['probability_up']:.1%}")
            print(f"  Confidence: {pred['confidence']:.1%}")
        print(f"  Model: {pred['model_used']}")

    pd.DataFrame(forward_predictions).T.to_csv('forward_predictions_5d.csv')
    print("\n‚úì Saved predictions to forward_predictions_5d.csv")

# ============================================================================
# SECTION 11: BACKTEST PERFORMANCE ANALYSIS
# ============================================================================

print("\n[11/12] Analyzing backtest performance...")

backtest_results = {}

for ticker in assets.keys():
    if ticker not in predictions:
        continue
    
    y_test = predictions[ticker]['y_test']
    y_pred = predictions[ticker]['y_pred']
    y_pred_proba = predictions[ticker]['y_pred_proba']
    test_dates = predictions[ticker]['X_test_index']
    
    accuracy = accuracy_score(y_test, y_pred)
    
    cm = confusion_matrix(y_test, y_pred)
    
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    else:
        precision = recall = f1 = 0
    
    try:
        roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None
    except:
        roc_auc = None
    
    test_returns = asset_closes[ticker].loc[test_dates].pct_change().shift(-5)
    test_returns = test_returns.iloc[:-5]
    y_pred_aligned = y_pred[:-5] if len(y_pred) > len(test_returns) else y_pred[:len(test_returns)]
    
    strategy_returns = test_returns * y_pred_aligned
    strategy_returns = strategy_returns.dropna()
    
    buy_hold_returns = test_returns.dropna()
    
    if len(strategy_returns) > 0:
        total_strategy_return = (1 + strategy_returns).prod() - 1
        total_buy_hold_return = (1 + buy_hold_returns).prod() - 1
        
        sharpe_strategy = strategy_returns.mean() / strategy_returns.std() * np.sqrt(252) if strategy_returns.std() > 0 else 0
        sharpe_buy_hold = buy_hold_returns.mean() / buy_hold_returns.std() * np.sqrt(252) if buy_hold_returns.std() > 0 else 0
        
        max_dd_strategy = (strategy_returns.cumsum() - strategy_returns.cumsum().cummax()).min()
        max_dd_buy_hold = (buy_hold_returns.cumsum() - buy_hold_returns.cumsum().cummax()).min()
        
        win_rate = (strategy_returns > 0).sum() / len(strategy_returns)
    else:
        total_strategy_return = 0
        total_buy_hold_return = 0
        sharpe_strategy = 0
        sharpe_buy_hold = 0
        max_dd_strategy = 0
        max_dd_buy_hold = 0
        win_rate = 0
    
    backtest_results[ticker] = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'roc_auc': roc_auc,
        'total_strategy_return': total_strategy_return * 100,
        'total_buy_hold_return': total_buy_hold_return * 100,
        'sharpe_strategy': sharpe_strategy,
        'sharpe_buy_hold': sharpe_buy_hold,
        'max_drawdown_strategy': max_dd_strategy * 100,
        'max_drawdown_buy_hold': max_dd_buy_hold * 100,
        'win_rate': win_rate,
        'outperformance': (total_strategy_return - total_buy_hold_return) * 100
    }

backtest_df = pd.DataFrame(backtest_results).T
backtest_df.to_csv('backtest_performance.csv')

print("\nBacktest Performance Summary:")
print("=" * 80)
print(backtest_df[['accuracy', 'sharpe_strategy', 'sharpe_buy_hold', 'outperformance']].to_string())

# ============================================================================
# SECTION 12: CORRELATION ANALYSIS & FINAL REPORTS
# ============================================================================

print("\n[12/12] Performing final analysis...")

# Correlation matrix
recent_returns = returns[list(assets.keys())].tail(60)
corr_matrix = recent_returns.corr()

print("\nCurrent 60-Day Correlation Matrix:")
print(corr_matrix.round(3).to_string())
corr_matrix.to_csv('asset_correlation_matrix.csv')

# Rolling correlations - NO USO
rolling_corr = {}

oil_ticker_name = oil_ticker if oil_ticker else 'CL=F'

pairs = [
    ('SPY', 'QQQ', 'Stocks Correlation'),
    ('SPY', 'GLD', 'Stocks vs Gold'),
]

if oil_ticker_name in returns.columns:
    pairs.extend([
        ('SPY', oil_ticker_name, 'Stocks vs Oil'),
        (oil_ticker_name, 'GLD', 'Oil vs Gold'),
    ])

pairs.append(('QQQ', 'GLD', 'Tech vs Gold'))

for ticker1, ticker2, label in pairs:
    if ticker1 in returns.columns and ticker2 in returns.columns:
        rolling_corr[label] = returns[ticker1].rolling(60).corr(returns[ticker2])

if rolling_corr:
    rolling_corr_df = pd.DataFrame(rolling_corr)
    rolling_corr_df.to_csv('rolling_correlations.csv')
    print("\nCurrent Rolling Correlations:")
    print(rolling_corr_df.tail(1).to_string())

# Market regime
current_regime = int(regime_features['regime_current'].iloc[-1])
regime_names = {0: 'Low Volatility (Bull)', 1: 'Normal', 2: 'High Volatility (Bear)'}
current_regime_name = regime_names.get(current_regime, 'Unknown')

print("\n" + "=" * 80)
print("MARKET REGIME & TRADING SIGNALS")
print("=" * 80)
print(f"\nCurrent Market Regime: {current_regime_name}")

# Trading signals
for ticker, pred in forward_predictions.items():
    signal = "üü¢ BUY" if pred['prediction_direction'] == 'UP' and pred['confidence'] > 0.65 else \
             "üî¥ SELL" if pred['prediction_direction'] == 'DOWN/FLAT' and pred['confidence'] > 0.65 else \
             "üü° HOLD"
    
    print(f"\n{signal} {ticker} ({assets[ticker]})")
    print(f"  Price: ${pred['current_price']:.2f}")
    print(f"  Direction: {pred['prediction_direction']} ({pred['confidence']:.1%} confidence)")
    print(f"  Model: {pred['model_used']}")

# Summary report
summary = {
    'analysis_timestamp': datetime.utcnow().isoformat(),
    'market_regime': current_regime_name,
    'oil_source': oil_source if oil_ticker_used else 'N/A',
    'oil_price': current_oil_price if oil_ticker_used else None,
    'assets_analyzed': list(assets.keys()),
    'predictions': {
        ticker: {
            'direction': forward_predictions[ticker]['prediction_direction'],
            'confidence': forward_predictions[ticker]['confidence'],
            'current_price': forward_predictions[ticker]['current_price']
        }
        for ticker in forward_predictions.keys()
    },
    'backtest_performance': {
        ticker: {
            'accuracy': backtest_results[ticker]['accuracy'],
            'sharpe_strategy': backtest_results[ticker]['sharpe_strategy'],
            'outperformance': backtest_results[ticker]['outperformance']
        }
        for ticker in backtest_results.keys()
    }
}

with open('analysis_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)

metadata = {
    'system': 'Multi-Asset Market Movement Prediction System V2',
    'version': '2.0 - CL=F Oil Futures',
    'timestamp_utc': pd.Timestamp.utcnow().isoformat(),
    'python_version': sys.version.split()[0],
    'assets_tracked': list(assets.keys()),
    'oil_ticker_used': oil_ticker_used,
    'features_per_asset': len(features_dict[list(assets.keys())[0]].columns),
    'models_used': ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM', 'Ensemble'],
    'prediction_horizons': list(horizons.keys()),
    'data_period': f"{start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}",
    'total_trading_days': len(all_closes)
}

with open('system_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

print("\n" + "=" * 80)
print("ANALYSIS COMPLETE!")
print("=" * 80)

print("\n‚úÖ Generated files:")
print("   - forward_predictions_5d.csv")
print("   - backtest_performance.csv")
print("   - asset_correlation_matrix.csv")
print("   - rolling_correlations.csv")
print("   - feature_importance_*.csv (per asset)")
print("   - analysis_summary.json")
print("   - system_metadata.json")

print(f"\nüí° Oil Data: Using {oil_source} at ${current_oil_price:.2f}/barrel")
print(f"üìä Market Regime: {current_regime_name}")
print("\n‚ö†Ô∏è  Disclaimer: For educational purposes only.")
print("    Always conduct your own research before trading.")
print("\nHappy Trading! üöÄüìä")

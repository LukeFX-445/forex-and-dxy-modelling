import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

!pip install yfinance arch pandas numpy scipy scikit-learn statsmodels fredapi xgboost lightgbm tensorflow keras --quiet

import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import sys
from scipy import stats
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.model_selection import train_test_split, cross_val_score
from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression
from statsmodels.tsa.stattools import adfuller, acf
import xgboost as xgb
import lightgbm as lgb
from arch import arch_model
import math

# ============================================================================
# SECTION 1: DATA COLLECTION - ASSETS & MACRO INDICATORS
# ============================================================================

print("=" * 80)
print("MULTI-ASSET MARKET MOVEMENT PREDICTION SYSTEM")
print("=" * 80)

print("\n[1/12] Collecting market data...")

end_date = datetime.utcnow()
start_date = end_date - timedelta(days=5*365)  # 5 years of data

# Define primary assets to predict
assets = {
    'SPY': 'S&P 500 ETF',
    'QQQ': 'NASDAQ-100 ETF',
    'USO': 'WTI Oil ETF',
    'GLD': 'Gold ETF'
}

# Also track actual indices for validation
validation_tickers = {
    '^GSPC': 'S&P 500 Index',
    '^IXIC': 'NASDAQ Composite',
    'CL=F': 'Crude Oil Futures',
    'GC=F': 'Gold Futures'
}

# Download primary asset data
print("  Fetching primary assets...")
asset_data = {}
for ticker, name in assets.items():
    try:
        data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=False)
        if not data.empty:
            asset_data[ticker] = data
            print(f"    ‚úì {name} ({ticker})")
    except Exception as e:
        print(f"    ‚úó Failed to fetch {ticker}: {e}")

# Download market indicators and related assets
print("\n  Fetching market indicators...")
indicators = {
    '^VIX': 'VIX (Volatility Index)',
    '^TNX': '10-Year Treasury Yield',
    'DX-Y.NYB': 'US Dollar Index',
    'TLT': 'Long-Term Treasury ETF',
    'HYG': 'High Yield Corporate Bond ETF',
    'LQD': 'Investment Grade Corporate Bond ETF',
    'EEM': 'Emerging Markets ETF',
    'IWM': 'Russell 2000 Small Cap ETF',
    'XLE': 'Energy Sector ETF',
    'XLF': 'Financial Sector ETF',
    'XLK': 'Technology Sector ETF',
    'GDX': 'Gold Miners ETF',
    'UNG': 'Natural Gas ETF',
    'BTC-USD': 'Bitcoin',
    'ETH-USD': 'Ethereum'
}

indicator_data = {}
for ticker, name in indicators.items():
    try:
        data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=False)
        if not data.empty:
            indicator_data[ticker] = data
            print(f"    ‚úì {name}")
    except:
        continue

print(f"\n‚úì Collected data for {len(asset_data)} primary assets and {len(indicator_data)} indicators")

# ============================================================================
# SECTION 2: DATA PREPROCESSING & FEATURE ENGINEERING
# ============================================================================

print("\n[2/12] Preprocessing and engineering features...")

# Combine all data into unified DataFrames
def extract_ohlcv(data_dict):
    """Extract OHLCV data from yfinance results"""
    # Use explicit dictionaries to collect Series objects
    closes_series = {}
    opens_series = {}
    highs_series = {}
    lows_series = {}
    volumes_series = {}

    all_indices = pd.Index([]) # Start with an empty index to collect all valid date indices

    for ticker, data in data_dict.items():
        if not data.empty and all(col in data.columns for col in ['Close', 'Open', 'High', 'Low', 'Volume']):
            # Explicitly construct Series to ensure consistency and type
            # This handles cases where data['Column'] might be interpreted as scalar.
            closes_series[ticker] = pd.Series(data['Close'].values.flatten(), index=data.index, dtype=float)
            opens_series[ticker] = pd.Series(data['Open'].values.flatten(), index=data.index, dtype=float)
            highs_series[ticker] = pd.Series(data['High'].values.flatten(), index=data.index, dtype=float)
            lows_series[ticker] = pd.Series(data['Low'].values.flatten(), index=data.index, dtype=float)
            volumes_series[ticker] = pd.Series(data['Volume'].values.flatten(), index=data.index, dtype=float)
            all_indices = all_indices.union(data.index)
        else:
            # Provide empty Series for missing or problematic data to ensure type consistency
            closes_series[ticker] = pd.Series(dtype=float)
            opens_series[ticker] = pd.Series(dtype=float)
            highs_series[ticker] = pd.Series(dtype=float)
            lows_series[ticker] = pd.Series(dtype=float)
            volumes_series[ticker] = pd.Series(dtype=float)

    # Now, explicitly pass all_indices to DataFrame constructor, letting it align series
    if not all_indices.empty:
        closes = pd.DataFrame(closes_series, index=all_indices)
        opens = pd.DataFrame(opens_series, index=all_indices)
        highs = pd.DataFrame(highs_series, index=all_indices)
        lows = pd.DataFrame(lows_series, index=all_indices)
        volumes = pd.DataFrame(volumes_series, index=all_indices)
    else:
        # If all_indices is empty, it means no data was successfully fetched for any ticker.
        all_tickers = data_dict.keys()
        closes = pd.DataFrame(columns=all_tickers)
        opens = pd.DataFrame(columns=all_tickers)
        highs = pd.DataFrame(columns=all_tickers)
        lows = pd.DataFrame(columns=all_tickers)
        volumes = pd.DataFrame(columns=all_tickers)

    return closes, opens, highs, lows, volumes

# Extract price data
asset_closes, asset_opens, asset_highs, asset_lows, asset_volumes = extract_ohlcv(asset_data)
indicator_closes, _, _, _, _ = extract_ohlcv(indicator_data)

# Verify we have data
if asset_closes.empty:
    raise ValueError("Failed to download any asset data. Please check your internet connection and try again.")

print(f"  ‚úì Extracted data for {len(asset_closes.columns)} assets")

# Align all data to common dates (business days)
all_closes = pd.concat([asset_closes, indicator_closes], axis=1)
all_closes = all_closes.ffill(limit=5).dropna(how='all')

# Calculate returns
returns = all_closes.pct_change().dropna()
log_returns = np.log(all_closes / all_closes.shift(1)).dropna()

print(f"‚úì Data spans {len(all_closes)} trading days")

# ============================================================================
# SECTION 3: TARGET VARIABLE CREATION
# ============================================================================

print("\n[3/12] Creating prediction targets...")

# Define prediction horizons
horizons = {
    '1d': 1,
    '3d': 3,
    '5d': 5,
    '10d': 10,
    '20d': 20
}

# Create targets for each asset
targets = {}

for ticker in assets.keys():
    targets[ticker] = {}

    for horizon_name, days in horizons.items():
        # Forward returns
        forward_return = asset_closes[ticker].pct_change(days).shift(-days)

        # Classification targets
        # Ternary: -1 (down >0.5%), 0 (flat ¬±0.5%), 1 (up >0.5%)
        targets[ticker][f'{horizon_name}_direction'] = pd.cut(
            forward_return * 100,
            bins=[-np.inf, -0.5, 0.5, np.inf],
            labels=[-1, 0, 1]
        ).astype(float)

        # Binary: 0 (down/flat), 1 (up)
        targets[ticker][f'{horizon_name}_binary'] = (forward_return > 0).astype(int)

        # Actual forward return for regression
        targets[ticker][f'{horizon_name}_return'] = forward_return

# Convert to DataFrames
for ticker in assets.keys():
    targets[ticker] = pd.DataFrame(targets[ticker])

print(f"‚úì Created targets for {len(horizons)} prediction horizons")

# ============================================================================
# SECTION 4: COMPREHENSIVE FEATURE ENGINEERING
# ============================================================================

print("\n[4/12] Engineering comprehensive features...")

features_dict = {}

for ticker in assets.keys():
    features = pd.DataFrame(index=asset_closes.index)
    price = asset_closes[ticker]

    # ========== PRICE-BASED FEATURES ==========

    # Returns (multiple timeframes)
    for period in [1, 2, 3, 5, 10, 20, 60]:
        features[f'return_{period}d'] = price.pct_change(period)
        features[f'log_return_{period}d'] = np.log(price / price.shift(period))

    # Moving averages
    for window in [5, 10, 20, 50, 100, 200]:
        features[f'sma_{window}'] = price.rolling(window).mean()
        features[f'price_to_sma_{window}'] = price / features[f'sma_{window}']
        features[f'sma_{window}_slope'] = features[f'sma_{window}'].diff(5)

    # Exponential moving averages
    for span in [12, 26, 50]:
        features[f'ema_{span}'] = price.ewm(span=span).mean()
        features[f'price_to_ema_{span}'] = price / features[f'ema_{span}']

    # Moving average crossovers
    features['sma_cross_50_200'] = features['sma_50'] - features['sma_200']
    features['ema_cross_12_26'] = features['ema_12'] - features['ema_26']

    # ========== MOMENTUM INDICATORS ==========

    # RSI (multiple periods)
    for period in [7, 14, 21, 28]:
        delta = price.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

    # MACD
    ema_12 = price.ewm(span=12).mean()
    ema_26 = price.ewm(span=26).mean()
    features['macd'] = ema_12 - ema_26
    features['macd_signal'] = features['macd'].ewm(span=9).mean()
    features['macd_hist'] = features['macd'] - features['macd_signal']

    # Stochastic Oscillator
    for period in [14, 21]:
        low_min = asset_lows[ticker].rolling(window=period).min()
        high_max = asset_highs[ticker].rolling(window=period).max()
        features[f'stoch_{period}'] = 100 * (price - low_min) / (high_max - low_min)
        features[f'stoch_{period}_ma'] = features[f'stoch_{period}'].rolling(3).mean()

    # Rate of Change (ROC)
    for period in [5, 10, 20]:
        features[f'roc_{period}'] = ((price - price.shift(period)) / price.shift(period)) * 100

    # ========== VOLATILITY INDICATORS ==========

    # Historical volatility (multiple windows)
    for window in [5, 10, 20, 60]:
        features[f'volatility_{window}d'] = returns[ticker].rolling(window).std() * np.sqrt(252)

    # Bollinger Bands
    for window in [20, 50]:
        sma = price.rolling(window).mean()
        std = price.rolling(window).std()
        features[f'bb_upper_{window}'] = sma + 2 * std
        features[f'bb_lower_{window}'] = sma - 2 * std
        features[f'bb_width_{window}'] = (features[f'bb_upper_{window}'] - features[f'bb_lower_{window}']) / sma
        features[f'bb_position_{window}'] = (price - features[f'bb_lower_{window}']) / (features[f'bb_upper_{window}'] - features[f'bb_lower_{window}'])

    # ATR (Average True Range)
    high = asset_highs[ticker]
    low = asset_lows[ticker]
    close_prev = price.shift(1)

    tr1 = high - low
    tr2 = abs(high - close_prev)
    tr3 = abs(low - close_prev)
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)

    for period in [7, 14, 21]:
        features[f'atr_{period}'] = tr.rolling(period).mean()
        features[f'atr_{period}_pct'] = features[f'atr_{period}'] / price

    # ========== VOLUME INDICATORS ==========

    volume = asset_volumes[ticker]

    # Volume moving averages
    for window in [5, 10, 20]:
        features[f'volume_sma_{window}'] = volume.rolling(window).mean()
        features[f'volume_ratio_{window}'] = volume / features[f'volume_sma_{window}']

    # On-Balance Volume (OBV)
    obv = (np.sign(price.diff()) * volume).fillna(0).cumsum()
    features['obv'] = obv
    features['obv_ema'] = obv.ewm(span=20).mean()

    # Volume-Price Trend
    features['vpt'] = (volume * ((price - price.shift(1)) / price.shift(1))).cumsum()

    # ========== PATTERN RECOGNITION ==========

    # Higher highs, higher lows (uptrend)
    features['higher_high'] = (high > high.shift(1)).astype(int)
    features['higher_low'] = (low > low.shift(1)).astype(int)
    features['uptrend_strength'] = features['higher_high'].rolling(5).sum() + features['higher_low'].rolling(5).sum()

    # Price channels
    features['channel_high_20'] = high.rolling(20).max()
    features['channel_low_20'] = low.rolling(20).min()
    features['channel_position'] = (price - features['channel_low_20']) / (features['channel_high_20'] - features['channel_low_20'])

    # Distance from 52-week high/low
    features['dist_from_52w_high'] = (price / high.rolling(252).max() - 1) * 100
    features['dist_from_52w_low'] = (price / low.rolling(252).min() - 1) * 100

    # ========== STATISTICAL FEATURES ==========

    # Skewness and Kurtosis of returns
    for window in [20, 60]:
        features[f'return_skew_{window}'] = returns[ticker].rolling(window).skew()
        features[f'return_kurt_{window}'] = returns[ticker].rolling(window).kurt()

    # Z-score (standardized price)
    for window in [20, 60]:
        mean = price.rolling(window).mean()
        std = price.rolling(window).std()
        features[f'zscore_{window}'] = (price - mean) / std

    features_dict[ticker] = features

print(f"‚úì Created {len(features_dict['SPY'].columns)} features per asset")

# ========== CROSS-ASSET FEATURES ==========

print("\n[5/12] Creating cross-asset and macro features...")

# Market-wide features
market_features = pd.DataFrame(index=all_closes.index)

# VIX features (fear gauge)
if '^VIX' in indicator_closes.columns:
    vix = indicator_closes['^VIX']
    market_features['vix'] = vix
    market_features['vix_sma_20'] = vix.rolling(20).mean()
    market_features['vix_change'] = vix.pct_change()
    market_features['vix_spike'] = (vix > vix.rolling(20).mean() + vix.rolling(20).std()).astype(int)

# Treasury yield features
if '^TNX' in indicator_closes.columns:
    tnx = indicator_closes['^TNX']
    market_features['yield_10y'] = tnx
    market_features['yield_10y_change'] = tnx.diff()
    market_features['yield_10y_roc'] = tnx.pct_change(20)

# Dollar Index features
if 'DX-Y.NYB' in indicator_closes.columns:
    dxy = indicator_closes['DX-Y.NYB']
    market_features['dxy'] = dxy
    market_features['dxy_return_20d'] = dxy.pct_change(20)
    market_features['dxy_sma_50'] = dxy.rolling(50).mean()

# Credit spreads (HYG vs LQD)
if 'HYG' in indicator_closes.columns and 'LQD' in indicator_closes.columns:
    hyg_ret = indicator_closes['HYG'].pct_change(20)
    lqd_ret = indicator_closes['LQD'].pct_change(20)
    market_features['credit_spread_proxy'] = hyg_ret - lqd_ret

# Market breadth (SPY vs IWM)
if 'IWM' in indicator_closes.columns:
    spy_ret = asset_closes['SPY'].pct_change(20)
    iwm_ret = indicator_closes['IWM'].pct_change(20)
    market_features['large_vs_small_cap'] = spy_ret - iwm_ret

# Sector rotations
sector_etfs = ['XLE', 'XLF', 'XLK']
for sector in sector_etfs:
    if sector in indicator_closes.columns:
        sector_ret = indicator_closes[sector].pct_change(20)
        market_features[f'{sector}_momentum'] = sector_ret

# Crypto correlation (risk-on/risk-off)
if 'BTC-USD' in indicator_closes.columns:
    btc_ret = indicator_closes['BTC-USD'].pct_change(20)
    market_features['btc_momentum'] = btc_ret

# Inter-asset correlations (rolling)
market_features['spy_qqq_corr'] = returns['SPY'].rolling(60).corr(returns['QQQ'])
market_features['spy_uso_corr'] = returns['SPY'].rolling(60).corr(returns['USO'])
market_features['spy_gld_corr'] = returns['SPY'].rolling(60).corr(returns['GLD'])
market_features['uso_gld_corr'] = returns['USO'].rolling(60).corr(returns['GLD'])
market_features['qqq_gld_corr'] = returns['QQQ'].rolling(60).corr(returns['GLD'])

# Add market features to each asset's feature set
for ticker in assets.keys():
    features_dict[ticker] = pd.concat([features_dict[ticker], market_features], axis=1)

print(f"‚úì Added {len(market_features.columns)} market-wide features")

# ============================================================================
# SECTION 5: REGIME DETECTION
# ============================================================================

print("\n[6/12] Detecting market regimes...")

regime_features = pd.DataFrame(index=all_closes.index)

# Detect regimes for S&P 500
try:
    spy_returns = returns['SPY'] * 100

    model = MarkovRegression(
        spy_returns.dropna(),
        k_regimes=3,
        switching_variance=True
    )

    res = model.fit(maxiter=100, disp=False, warn_convergence=False)

    # Get regime probabilities
    smoothed = res.smoothed_marginal_probabilities
    regime_features['regime_0_prob'] = smoothed[0]
    regime_features['regime_1_prob'] = smoothed[1]
    regime_features['regime_2_prob'] = smoothed[2]
    regime_features['regime_current'] = smoothed.idxmax(axis=1)

    # Regime volatilities
    for i in range(3):
        vol = np.sqrt(res.params[f'sigma2[{i}]'])
        regime_features[f'regime_{i}_vol'] = vol

    print("  ‚úì Markov Regime-Switching model fitted")

except Exception as e:
    print(f"  Note: Regime detection using simpler method: {e}")
    # Fallback: volatility-based regimes
    vol = returns['SPY'].rolling(20).std()
    vol_z = (vol - vol.rolling(60).mean()) / vol.rolling(60).std()

    regime_features['regime_current'] = pd.cut(
        vol_z,
        bins=[-np.inf, -0.5, 0.5, np.inf],
        labels=[0, 1, 2]
    ).astype(float)

# Add regime features to all assets
for ticker in assets.keys():
    features_dict[ticker] = pd.concat([features_dict[ticker], regime_features], axis=1)

print(f"‚úì Added regime detection features")

# ============================================================================
# SECTION 6: PREPARE ML DATASETS
# ============================================================================

print("\n[7/12] Preparing machine learning datasets...")

# Function to create ML-ready dataset
def prepare_ml_dataset(ticker, horizon='5d', target_type='binary'):
    """Prepare features and targets for ML training"""

    X = features_dict[ticker].copy()
    y = targets[ticker][f'{horizon}_{target_type}']

    # Align X and y by their index
    common_index = X.index.intersection(y.index)
    X = X.loc[common_index]
    y = y.loc[common_index]

    # Drop rows where target is NaN (cannot train on missing target)
    valid_mask = y.notna()
    X_clean = X[valid_mask]
    y_clean = y[valid_mask]

    # Handle infinities and NaNs in features
    X_clean = X_clean.replace([np.inf, -np.inf], np.nan)
    
    # Impute NaNs in features more robustly
    if not X_clean.empty and not X_clean.columns.empty: # Check for empty columns too
        # Calculate medians for each column
        column_medians = X_clean.median()
        
        # Fill NaNs with their respective column medians
        X_clean = X_clean.fillna(column_medians)
        
        # Drop columns that are entirely NaN after initial feature generation (e.g., if a feature was all NaNs)
        X_clean = X_clean.dropna(axis=1, how='all')
        
        # Fill any remaining NaNs (shouldn't be many after median fill and dropping all-NaN columns) with 0
        X_clean = X_clean.fillna(0)
    else:
        return pd.DataFrame(), pd.Series()

    # After cleaning features, ensure X_clean is not empty and align y_clean
    if X_clean.empty:
        return pd.DataFrame(), pd.Series()
        
    y_clean = y_clean.loc[X_clean.index] # Ensure y_clean matches X_clean's index after final cleaning
    
    # Final check for empty after all cleaning
    if y_clean.empty:
        return pd.DataFrame(), pd.Series()

    return X_clean, y_clean

# Prepare datasets for all assets (5-day binary prediction as default)
ml_datasets = {}

for ticker in assets.keys():
    X, y = prepare_ml_dataset(ticker, horizon='5d', target_type='binary')
    ml_datasets[ticker] = {'X': X, 'y': y}
    print(f"  ‚úì {ticker}: {len(X)} samples, {X.shape[1]} features")

print(f"\n‚úì Prepared ML datasets for {len(ml_datasets)} assets")

# ============================================================================
# SECTION 7: TRAIN ML MODELS
# ============================================================================

print("\n[8/12] Training machine learning models...")

models = {}
predictions = {}
performance = {}

for ticker in assets.keys():
    print(f"\n  Training models for {ticker} ({assets[ticker]})...")

    X = ml_datasets[ticker]['X']
    y = ml_datasets[ticker]['y']

    # Use walk-forward validation (time series split)
    train_size = int(len(X) * 0.7)
    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]
    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]

    # Check for empty dataframes after split
    if X_train.empty or y_train.empty or X_test.empty or y_test.empty:
        print(f"    ‚úó Skipping {ticker}: Insufficient data for training/testing after split.")
        continue

    # Scale features
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Train multiple models
    model_dict = {}

    # 1. Logistic Regression (baseline)
    try:
        lr = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
        lr.fit(X_train_scaled, y_train)
        model_dict['LogisticRegression'] = lr
    except:
        pass

    # 2. Random Forest
    try:
        rf = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        )
        rf.fit(X_train_scaled, y_train)
        model_dict['RandomForest'] = rf
    except:
        pass

    # 3. Gradient Boosting
    try:
        gb = GradientBoostingClassifier(
            n_estimators=200,
            max_depth=5,
            learning_rate=0.05,
            subsample=0.8,
            random_state=42
        )
        gb.fit(X_train_scaled, y_train)
        model_dict['GradientBoosting'] = gb
    except:
        pass

    # 4. XGBoost
    try:
        xgb_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )
        xgb_model.fit(X_train_scaled, y_train)
        model_dict['XGBoost'] = xgb_model
    except:
        pass

    # 5. LightGBM
    try:
        lgb_model = lgb.LGBMClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            verbose=-1
        )
        lgb_model.fit(X_train_scaled, y_train)
        model_dict['LightGBM'] = lgb_model
    except:
        pass

    # Ensemble voting classifier
    if len(model_dict) >= 3:
        voting_clf = VotingClassifier(
            estimators=[(name, model) for name, model in model_dict.items()],
            voting='soft'
        )
        voting_clf.fit(X_train_scaled, y_train)
        model_dict['Ensemble'] = voting_clf

    # Evaluate models
    results = {}
    for name, model in model_dict.items():
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None

        accuracy = accuracy_score(y_test, y_pred)

        results[name] = {
            'accuracy': accuracy,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }

        print(f"    {name}: {accuracy:.2%} accuracy")

    # Select best model
    best_model_name = max(results, key=lambda x: results[x]['accuracy']) if results else None
    if best_model_name:
        best_model = model_dict[best_model_name]
        print(f"    ‚Üí Best: {best_model_name} ({results[best_model_name]['accuracy']:.2%})")

        # Store
        models[ticker] = {
            'scaler': scaler,
            'best_model': best_model,
            'best_model_name': best_model_name,
            'all_models': model_dict,
            'feature_names': X.columns.tolist()
        }

        predictions[ticker] = {
            'y_test': y_test,
            'y_pred': results[best_model_name]['y_pred'],
            'y_pred_proba': results[best_model_name]['y_pred_proba'],
            'X_test_index': X_test.index
        }

        performance[ticker] = {
            'accuracy': results[best_model_name]['accuracy'],
            'model_used': best_model_name
        }
    else:
        print(f"    ‚úó No models trained for {ticker} (no valid models or insufficient data).")

print(f"\n‚úì Trained models for all {len(models)} assets")

# ============================================================================
# SECTION 8: FEATURE IMPORTANCE ANALYSIS
# ============================================================================

print("\n[9/12] Analyzing feature importance...")

feature_importance_dict = {}

for ticker in assets.keys():
    if ticker not in models: # Skip if model training failed for this ticker
        print(f"  ‚úó Skipping feature importance for {ticker}: No model trained.")
        continue
    model = models[ticker]['best_model']
    feature_names = models[ticker]['feature_names']

    # Get feature importance (works for tree-based models)
    if hasattr(model, 'feature_importances_'):
        importance = model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)

        feature_importance_dict[ticker] = feature_importance_df

        print(f"\n  Top 10 features for {ticker}:")
        print(feature_importance_df.head(10)[['feature', 'importance']].to_string(index=False))

# Save feature importance
for ticker, df in feature_importance_dict.items():
    df.to_csv(f'feature_importance_{ticker}.csv', index=False)

# ============================================================================
# SECTION 9: GENERATE PREDICTIONS FOR NEXT 5 DAYS
# ============================================================================

print("\n[10/12] Generating forward predictions...")

forward_predictions = {}

for ticker in assets.keys():
    if ticker not in models: # Skip if model training failed for this ticker
        print(f"  ‚úó Skipping forward prediction for {ticker}: No model trained.")
        continue

    # Get the feature names the scaler was fitted on (from the model object)
    fitted_feature_names = models[ticker]['feature_names']

    # Get latest features
    latest_features_raw = features_dict[ticker].iloc[-1:].copy()
    
    # Reindex latest_features_raw to ensure it has the exact same columns as the training data
    latest_features = latest_features_raw.reindex(columns=fitted_feature_names, fill_value=np.nan)
    
    # Fill any NaNs that might have been introduced by reindexing or were already there.
    # Use the medians from the X_clean data (ml_datasets[ticker]['X']) for consistency.
    training_medians = ml_datasets[ticker]['X'].median()
    latest_features = latest_features.fillna(training_medians)
    
    # Ensure no remaining NaNs (e.g., if a new column was all NaN and not covered by median) by filling with 0
    latest_features = latest_features.fillna(0)

    # Scale
    scaler = models[ticker]['scaler']
    latest_scaled = scaler.transform(latest_features)

    # Predict
    model = models[ticker]['best_model']
    pred_class = model.predict(latest_scaled)[0]
    pred_proba = model.predict_proba(latest_scaled)[0] if hasattr(model, 'predict_proba') else None

    current_price = float(asset_closes[ticker].iloc[-1])

    forward_predictions[ticker] = {
        'current_price': current_price,
        'prediction_class': int(pred_class),
        'prediction_direction': 'UP' if pred_class == 1 else 'DOWN/FLAT',
        'probability_up': float(pred_proba[1]) if pred_proba is not None else None,
        'probability_down': float(pred_proba[0]) if pred_proba is not None else None,
        'confidence': float(max(pred_proba)) if pred_proba is not None else None,
        'model_used': models[ticker]['best_model_name']
    }

# Display predictions
print("\n" + "=" * 80)
print("PREDICTIONS FOR NEXT 5 DAYS")
print("=" * 80)

for ticker, pred in forward_predictions.items():
    print(f"\n{ticker} ({assets[ticker]}):")
    print(f"  Current Price: ${pred['current_price']:.2f}")
    print(f"  Prediction: {pred['prediction_direction']}")
    print(f"  Probability UP: {pred['probability_up']:.1%}")
    print(f"  Confidence: {pred['confidence']:.1%}")
    print(f"  Model: {pred['model_used']}")

# Save predictions
pd.DataFrame(forward_predictions).T.to_csv('forward_predictions_5d.csv')

# ============================================================================
# SECTION 10: BACKTEST PERFORMANCE ANALYSIS
# ============================================================================

print("\n[11/12] Analyzing backtest performance...")

backtest_results = {}

for ticker in assets.keys():
    if ticker not in models: # Skip if model training failed for this ticker
        print(f"  ‚úó Skipping backtest analysis for {ticker}: No model trained.")
        continue
    y_test = predictions[ticker]['y_test']
    y_pred = predictions[ticker]['y_pred']
    y_pred_proba = predictions[ticker]['y_pred_proba']
    test_dates = predictions[ticker]['X_test_index']

    # Classification metrics
    accuracy = accuracy_score(y_test, y_pred)

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # True positives, false positives, etc.
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    else:
        precision = recall = f1 = 0

    # ROC AUC
    try:
        roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None
    except:
        roc_auc = None

    # Trading simulation
    test_returns = asset_closes[ticker].loc[test_dates].pct_change().shift(-5)  # 5-day forward
    test_returns = test_returns.iloc[:-5]  # Remove last 5 days
    y_pred_aligned = y_pred[:-5] if len(y_pred) > len(test_returns) else y_pred[:len(test_returns)]

    # Strategy returns: go long when predicting up, flat when predicting down
    strategy_returns = test_returns * y_pred_aligned
    strategy_returns = strategy_returns.dropna()

    # Buy and hold returns
    buy_hold_returns = test_returns.dropna()

    # Performance metrics
    if len(strategy_returns) > 0:
        total_strategy_return = (1 + strategy_returns).prod() - 1
        total_buy_hold_return = (1 + buy_hold_returns).prod() - 1

        sharpe_strategy = strategy_returns.mean() / strategy_returns.std() * np.sqrt(252) if strategy_returns.std() > 0 else 0
        sharpe_buy_hold = buy_hold_returns.mean() / buy_hold_returns.std() * np.sqrt(252) if buy_hold_returns.std() > 0 else 0

        max_dd_strategy = (strategy_returns.cumsum() - strategy_returns.cumsum().cummax()).min()
        max_dd_buy_hold = (buy_hold_returns.cumsum() - buy_hold_returns.cumsum().cummax()).min()

        win_rate = (strategy_returns > 0).sum() / len(strategy_returns)
    else:
        total_strategy_return = 0
        total_buy_hold_return = 0
        sharpe_strategy = 0
        sharpe_buy_hold = 0
        max_dd_strategy = 0
        max_dd_buy_hold = 0
        win_rate = 0

    backtest_results[ticker] = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'roc_auc': roc_auc,
        'total_strategy_return': total_strategy_return * 100,
        'total_buy_hold_return': total_buy_hold_return * 100,
        'sharpe_strategy': sharpe_strategy,
        'sharpe_buy_hold': sharpe_buy_hold,
        'max_drawdown_strategy': max_dd_strategy * 100,
        'max_drawdown_buy_hold': max_dd_buy_hold * 100,
        'win_rate': win_rate,
        'outperformance': (total_strategy_return - total_buy_hold_return) * 100
    }

backtest_df = pd.DataFrame(backtest_results).T
backtest_df.to_csv('backtest_performance.csv')

print("\nBacktest Performance Summary:")
print("=" * 80)
if not backtest_df.empty:
    print(backtest_df[['accuracy', 'sharpe_strategy', 'sharpe_buy_hold', 'outperformance']].to_string())
else:
    print("No backtest results available as no models were trained.")

# ============================================================================
# SECTION 11: CROSS-ASSET CORRELATION & INTERMARKET ANALYSIS
# ============================================================================

print("\n[12/12] Performing cross-asset correlation analysis...")

# Current correlation matrix
recent_returns = returns[list(assets.keys())].tail(60)
corr_matrix = recent_returns.corr()

print("\nCurrent 60-Day Correlation Matrix:")
print(corr_matrix.round(3).to_string())
corr_matrix.to_csv('asset_correlation_matrix.csv')

# Rolling correlations
rolling_corr = {}
pairs = [
    ('SPY', 'QQQ', 'Stocks Correlation'),
    ('SPY', 'GLD', 'Stocks vs Gold'),
    ('SPY', 'USO', 'Stocks vs Oil'),
    ('USO', 'GLD', 'Oil vs Gold'),
    ('QQQ', 'GLD', 'Tech vs Gold')
]

for ticker1, ticker2, label in pairs:
    rolling_corr[label] = returns[ticker1].rolling(60).corr(returns[ticker2])

rolling_corr_df = pd.DataFrame(rolling_corr)
rolling_corr_df.to_csv('rolling_correlations.csv')

print("\nCurrent Rolling Correlations:")
print(rolling_corr_df.tail(1).to_string())

# ============================================================================
# SECTION 12: REGIME-BASED STRATEGY SIGNALS
# ============================================================================

print("\n" + "=" * 80)
print("REGIME-BASED TRADING SIGNALS")
print("=" * 80)

# Get current regime
current_regime = int(regime_features['regime_current'].iloc[-1])
regime_names = {0: 'Low Volatility (Bull)', 1: 'Normal', 2: 'High Volatility (Bear)'}
current_regime_name = regime_names.get(current_regime, 'Unknown')

print(f"\nCurrent Market Regime: {current_regime_name}")

# Regime-specific recommendations
regime_strategies = {
    0: {  # Bull regime
        'recommended_assets': ['SPY', 'QQQ'],
        'avoid_assets': [],
        'strategy': 'Growth-focused, increase equity exposure',
        'risk_level': 'Moderate-High'
    },
    1: {  # Normal regime
        'recommended_assets': ['SPY', 'QQQ', 'GLD'],
        'avoid_assets': [],
        'strategy': 'Balanced allocation across asset classes',
        'risk_level': 'Moderate'
    },
    2: {  # Bear regime
        'recommended_assets': ['GLD', 'USO'],
        'avoid_assets': ['QQQ'],
        'strategy': 'Defensive positioning, favor safe havens',
        'risk_level': 'Low-Moderate'
    }
}

current_strategy = regime_strategies.get(current_regime, regime_strategies[1])

print(f"\nRecommended Strategy: {current_strategy['strategy']}")
print(f"Risk Level: {current_strategy['risk_level']}")
print(f"Favor: {', '.join(current_strategy['recommended_assets'])}")
if current_strategy['avoid_assets']:
    print(f"Avoid: {', '.join(current_strategy['avoid_assets'])}")

# ============================================================================
# SECTION 13: COMPREHENSIVE TRADING SIGNALS
# ============================================================================

print("\n" + "=" * 80)
print("COMPREHENSIVE TRADING SIGNALS")
print("=" * 80)

trading_signals = {}

for ticker in assets.keys():
    # Skip if model training failed for this ticker
    if ticker not in models:
        print(f"  ‚úó Skipping trading signals for {ticker}: No model trained.")
        continue

    # Get latest technical indicators
    latest_features = features_dict[ticker].iloc[-1]

    # Prediction signal
    pred_signal = forward_predictions[ticker]['prediction_direction']
    confidence = forward_predictions[ticker]['confidence'] # Corrected

    # Technical signals
    rsi_14 = latest_features.get('rsi_14', 50.0) # Ensure float default
    if pd.isna(rsi_14): rsi_14 = 50.0

    macd_hist = latest_features.get('macd_hist', 0.0) # Ensure float default
    if pd.isna(macd_hist): macd_hist = 0.0

    price_to_sma_50 = latest_features.get('price_to_sma_50', 1.0) # Ensure float default
    if pd.isna(price_to_sma_50): price_to_sma_50 = 1.0

    bb_position_20 = latest_features.get('bb_position_20', 0.5) # Ensure float default
    if pd.isna(bb_position_20): bb_position_20 = 0.5

    # Determine overall signal strength
    signals = []

    # ML prediction
    if pred_signal == 'UP':
        signals.append(confidence)
    else:
        signals.append(-(1 - confidence))

    # RSI signal
    if rsi_14 < 30:
        signals.append(0.3)  # Oversold - bullish
    elif rsi_14 > 70:
        signals.append(-0.3)  # Overbought - bearish
    else:
        signals.append(0)

    # MACD signal
    if macd_hist > 0:
        signals.append(0.2)
    else:
        signals.append(-0.2)

    # Trend signal (price vs SMA)
    if price_to_sma_50 > 1.02:
        signals.append(0.2)
    elif price_to_sma_50 < 0.98:
        signals.append(-0.2)
    else:
        signals.append(0)

    # Bollinger Band signal
    if bb_position_20 < 0.2:
        signals.append(0.2)  # Near lower band - bullish
    elif bb_position_20 > 0.8:
        signals.append(-0.2)  # Near upper band - bearish
    else:
        signals.append(0)

    # Composite signal
    composite_signal = np.mean(signals)

    # Determine action
    if composite_signal > 0.3:
        action = 'STRONG BUY'
        color = 'üü¢'
    elif composite_signal > 0.1:
        action = 'BUY'
        color = 'üü¢'
    elif composite_signal < -0.3:
        action = 'STRONG SELL'
        color = 'üî¥'
    elif composite_signal < -0.1:
        action = 'SELL'
        color = 'üî¥'
    else:
        action = 'HOLD'
        color = 'üü°'

    trading_signals[ticker] = {
        'action': action,
        'signal_strength': composite_signal,
        'ml_prediction': pred_signal,
        'ml_confidence': forward_predictions[ticker]['confidence'],
        'rsi_14': rsi_14,
        'macd_hist': macd_hist,
        'trend': 'Up' if price_to_sma_50 > 1 else 'Down',
        'current_price': forward_predictions[ticker]['current_price']
    }

    print(f"\n{color} {ticker} ({assets[ticker]}):")
    print(f"   Action: {action}")
    print(f"   Signal Strength: {composite_signal:.2f}")
    print(f"   ML Prediction: {pred_signal} ({forward_predictions[ticker]['confidence']:.1%} confidence)")
    print(f"   RSI: {rsi_14:.1f}")
    print(f"   Trend: {trading_signals[ticker]['trend']}")
    print(f"   Current Price: ${trading_signals[ticker]['current_price']:.2f}")

# Save trading signals
pd.DataFrame(trading_signals).T.to_csv('trading_signals_comprehensive.csv')

# ============================================================================
# SECTION 14: RISK MANAGEMENT RECOMMENDATIONS
# ============================================================================

print("\n" + "=" * 80)
print("RISK MANAGEMENT RECOMMENDATIONS")
print("=" * 80)

for ticker in assets.keys():
    if ticker not in models: # Skip if model training failed for this ticker
        print(f"  ‚úó Skipping risk management for {ticker}: No model trained.")
        continue
    current_price = forward_predictions[ticker]['current_price']

    # Get volatility
    vol = features_dict[ticker]['volatility_20d'].iloc[-1] / 100  # Convert to fraction

    # Calculate position sizing based on volatility
    account_risk = 0.02  # Risk 2% of account per trade

    # Stop loss at 2x ATR
    # Ensure atr is a scalar; default to 0.02 * current_price if it's not available or a Series
    atr_val = features_dict[ticker].get('atr_14', features_dict[ticker].get('atr_7'))
    if isinstance(atr_val, pd.Series) and not atr_val.empty:
        atr = atr_val.iloc[-1]
    elif isinstance(atr_val, (float, np.number)):
        atr = atr_val
    else:
        atr = current_price * 0.02 # Fallback if atr calculation fails or is NaN
    
    # Handle potential NaN in atr_val.iloc[-1] if the series is all NaNs
    if pd.isna(atr):
        atr = current_price * 0.02 # Fallback to a reasonable default

    stop_loss_distance = 2 * atr
    stop_loss_price = current_price - stop_loss_distance
    stop_loss_pct = (stop_loss_distance / current_price) * 100

    # Take profit at 3x ATR (1.5 risk-reward ratio)
    take_profit_distance = 3 * atr
    take_profit_price = current_price + take_profit_distance
    take_profit_pct = (take_profit_distance / current_price) * 100

    # Position size calculation
    # Risk per share = stop loss distance
    # Position size = (Account Value * Risk %) / Risk per share
    # Assuming $100,000 account
    account_value = 100000
    if stop_loss_distance > 0:
        position_size = (account_value * account_risk) / stop_loss_distance
    else:
        position_size = 0 # Avoid division by zero
    
    position_value = position_size * current_price
    position_pct = (position_value / account_value) * 100

    print(f"\n{ticker} ({assets[ticker]}):")
    print(f"  Current Price: ${current_price:.2f}")
    print(f"  Stop Loss: ${stop_loss_price:.2f} (-{stop_loss_pct:.1f}%)")
    print(f"  Take Profit: ${take_profit_price:.2f} (+{take_profit_pct:.1f}%)")
    print(f"  Suggested Position Size: {position_size:.0f} shares (${position_value:.0f}, {position_pct:.1f}% of account)")
    print(f"  Daily Volatility: {vol*100:.1f}%")

# ============================================================================
# SECTION 15: MULTI-HORIZON PREDICTIONS
# ============================================================================

print("\n" + "=" * 80)
print("MULTI-HORIZON PREDICTIONS")
print("=" * 80)

multi_horizon_predictions = {}

for ticker in assets.keys():
    multi_horizon_predictions[ticker] = {}

    for horizon in ['1d', '3d', '5d', '10d', '20d']:
        try:
            # Prepare dataset for this horizon
            X, y = prepare_ml_dataset(ticker, horizon=horizon, target_type='binary')

            # Skip if dataset is empty
            if X.empty or y.empty:
                print(f"  ‚úó Skipping multi-horizon prediction for {ticker} ({horizon}): Insufficient data.")
                continue

            # Use last 70% for training
            train_size = int(len(X) * 0.7)
            X_train = X.iloc[:train_size]
            y_train = y.iloc[:train_size]
            
            # Ensure train data is not empty
            if X_train.empty or y_train.empty:
                print(f"  ‚úó Skipping multi-horizon prediction for {ticker} ({horizon}): Insufficient training data.")
                continue

            # Scale
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train)

            # Predict latest
            latest_features_raw = features_dict[ticker].iloc[-1:].copy()
            
            # Get the feature names for this horizon's prepared dataset
            fitted_feature_names_for_horizon = X.columns

            # Reindex latest_features_raw to ensure it has the exact same columns
            latest_features = latest_features_raw.reindex(columns=fitted_feature_names_for_horizon, fill_value=np.nan)

            # Fill any NaNs that might have been introduced by reindexing or were already there.
            # Use the medians from the X_clean data (ml_datasets[ticker]['X']) for consistency.
            training_medians = ml_datasets[ticker]['X'].median()
            latest_features = latest_features.fillna(training_medians)

            # Ensure no remaining NaNs (e.g., if a new column was all NaN and not covered by median) by filling with 0
            latest_features = latest_features.fillna(0)

            latest_scaled = scaler.transform(latest_features)

            pred_class = model.predict(latest_scaled)[0]
            pred_proba = model.predict_proba(latest_scaled)[0]

            multi_horizon_predictions[ticker][horizon] = {
                'direction': 'UP' if pred_class == 1 else 'DOWN',
                'probability_up': float(pred_proba[1]),
                'confidence': float(max(pred_proba))
            }
        except Exception as e:
            print(f"  ‚úó Error during multi-horizon prediction for {ticker} ({horizon}): {e}")
            continue

print("\nPrediction Confidence Across Horizons:")
for ticker in assets.keys():
    print(f"\n{ticker}:")
    for horizon in ['1d', '3d', '5d', '10d', '20d']:
        if horizon in multi_horizon_predictions.get(ticker, {}):
            pred = multi_horizon_predictions[ticker][horizon]
            print(f"  {horizon}: {pred['direction']} ({pred['confidence']:.1%} confidence)")

# Save multi-horizon predictions
multi_horizon_df = pd.DataFrame({
    ticker: {
        f"{horizon}_{key}": multi_horizon_predictions[ticker][horizon][key]
        for horizon in multi_horizon_predictions.get(ticker, {})
        for key in multi_horizon_predictions[ticker][horizon]
    }
    for ticker in multi_horizon_predictions
}).T

multi_horizon_df.to_csv('multi_horizon_predictions.csv')

# ============================================================================
# SECTION 16: EXPORT SUMMARY & METADATA
# ============================================================================

print("\n" + "=" * 80)
print("GENERATING FINAL REPORTS")
print("=" * 80)

# Filter relevant data for summary if models were not trained for all assets
filtered_forward_predictions = {k: v for k, v in forward_predictions.items() if k in models}
filtered_trading_signals = {k: v for k, v in trading_signals.items() if k in models}
filtered_backtest_results = {k: v for k, v in backtest_results.items() if k in models}

summary = {
    'analysis_timestamp': datetime.utcnow().isoformat(),
    'market_regime': current_regime_name,
    'regime_code': int(current_regime),
    'assets_analyzed': list(assets.keys()),
    'predictions': {
        ticker: {
            'direction': pred['prediction_direction'],
            'confidence': pred['confidence'],
            'current_price': pred['current_price']
        }
        for ticker, pred in filtered_forward_predictions.items()
    } if filtered_forward_predictions else {},
    'trading_signals': {
        ticker: signal['action']
        for ticker, signal in filtered_trading_signals.items()
    } if filtered_trading_signals else {},
    'backtest_performance': {
        ticker: {
            'accuracy': res['accuracy'],
            'sharpe_strategy': res['sharpe_strategy'],
            'outperformance': res['outperformance']
        }
        for ticker, res in filtered_backtest_results.items()
    } if filtered_backtest_results else {},
    'top_pick': max(filtered_forward_predictions.items(), key=lambda x: x[1]['confidence'])[0] if filtered_forward_predictions else 'N/A',
    'most_bullish': max(filtered_trading_signals.items(), key=lambda x: x[1]['signal_strength'])[0] if filtered_trading_signals else 'N/A',
    'most_bearish': min(filtered_trading_signals.items(), key=lambda x: x[1]['signal_strength'])[0] if filtered_trading_signals else 'N/A'
}

with open('analysis_summary.json', 'w') as f:
    json.dump(summary, f, indent=2)

# Metadata
metadata = {
    'system': 'Multi-Asset Market Movement Prediction System',
    'version': '1.0',
    'timestamp_utc': pd.Timestamp.utcnow().isoformat(),
    'python_version': sys.version.split()[0],
    'assets_tracked': list(assets.keys()),
    'features_per_asset': len(features_dict['SPY'].columns) if 'SPY' in features_dict and not features_dict['SPY'].empty else 0,
    'models_used': ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM', 'Ensemble'],
    'prediction_horizons': list(horizons.keys()),
    'data_period': f"{start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}",
    'total_trading_days': len(all_closes)
}

with open('system_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

print("\n‚úÖ Analysis complete! Generated files:")
print("   - forward_predictions_5d.csv")
print("   - backtest_performance.csv")
print("   - asset_correlation_matrix.csv")
print("   - rolling_correlations.csv")
print("   - trading_signals_comprehensive.csv")
print("   - multi_horizon_predictions.csv")
print("   - feature_importance_*.csv (per asset)")
print("   - analysis_summary.json")
print("   - system_metadata.json")

print("\n" + "=" * 80)
print("FINAL RECOMMENDATIONS")
print("=" * 80)

print(f"\nüéØ Market Regime: {current_regime_name}")
print(f"üìä Strategy: {current_strategy['strategy']}")

if summary['top_pick'] != 'N/A':
    print(f"\nüèÜ Highest Confidence Prediction: {summary['top_pick']}")
    print(f"   ‚Üí {forward_predictions[summary['top_pick']]['prediction_direction']} with {forward_predictions[summary['top_pick']]['confidence']:.1%} confidence")

if summary['most_bullish'] != 'N/A':
    print(f"\nüêÇ Most Bullish: {summary['most_bullish']}")
    print(f"   ‚Üí Signal Strength: {trading_signals[summary['most_bullish']]['signal_strength']:.2f}")

if summary['most_bearish'] != 'N/A':
    print(f"\nüêª Most Bearish: {summary['most_bearish']}")
    print(f"   ‚Üí Signal Strength: {trading_signals[summary['most_bearish']]['signal_strength']:.2f}")

if filtered_backtest_results: # Only print if there's any backtest result
    print("\nüìà Best Backtest Performer:")
    best_performer = max(filtered_backtest_results.items(), key=lambda x: x[1]['sharpe_strategy'])
    print(f"   {best_performer[0]}: Sharpe {best_performer[1]['sharpe_strategy']:.2f}, Outperformance {best_performer[1]['outperformance']:.2f}%")
else:
    print("\nüìà No backtest performance available to rank.")

print("\n" + "=" * 80)
print("MULTI-ASSET FORECASTING SYSTEM - COMPLETE")
print("=" * 80)
print("\n‚ö†Ô∏è  Disclaimer: This is for educational purposes only.")
print("    Past performance does not guarantee future results.")
print("    Always conduct your own research before trading.")
print("\nHappy Trading! üöÄüìä")
